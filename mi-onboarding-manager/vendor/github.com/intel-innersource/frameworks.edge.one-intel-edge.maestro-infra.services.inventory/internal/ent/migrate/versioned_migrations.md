<!---
  SPDX-FileCopyrightText: (C) 2022 Intel Corporation
  SPDX-License-Identifier: LicenseRef-Intel

  ------------------------------------------------------

  This document is adapted from https://github.com/intel-innersource/frameworks.edge.one-intel-edge.maestro-app.application-catalog/blob/main/docs/migrations.md
-->
# Database Schema Migration

The inventory service uses the [entgo.io] ORM, which supports both automatic
migration and versioned migration.

While automatic migration is easy, it is also limited to relatively simple
schema changes. In order to provide a more robust migration capabilities, the
inventory service is using the version migration approach, which is facilitated
by entgo ORM and [Atlas] - an open-source database schema management system.

Both entgo and Atlas work together to provide the tooling for projects to make
versioned schema migrations relatively easy and streamlined. There are two
high-level stages in this process:

1) Generating migrations (at development time)
2) Applying migrations (at deployment time)

The entgo ORM framework supports DB schema generation from Go code or Protobuf,
which also allows it to generate change records that track how the schema
changes and from that generated versioned migration files.

## Generating Migrations
There are several phases in this stage. Each is briefly described in the
sections below in concrete terms of how it applies to the Inventory service
project.

### Modifying the Databases Schema
When the developer makes changes to the Protobuf schema definitions, they will
usually run the following to regenerate the ent schema and Go code:

```bash
make buf-gen && make ent-gen
```

After that, other code changes can be made to implement a new feature or fix.

### Generating Migration Scripts/Code

Once the code modifications are done, the developer also needs generate
a SQL migration accompanying the schema change, before submitting a patch.
This is accomplished using the following command, where the `migration-name`
parameter gives the aggregated set of schema changes a name.

```bash
make migration-generate MIGRATION=<migration-name>
```

Running this command, will bring up a temporary Postgres database instance,
which will be used to replay the entire migration history to compute the current
schema state and to produce a set of SQL commands to migrate the schema to the
new/future state. These commands, along with hashes will be stored in
the `internal/ent/migrate/migrations` directory, effectively extending the
schema evolution records and thus supporting future migrations. The generated
migration records as well as the generated SQL commands must be checked-in, as
they become part of the Docker image.

### Data-dependent Migrations

Sometimes, the migrations automatically generated in the previous step are not
sufficient to actually migrate a live database with data in it. The [linter](#validating-generated-scripts)
should catch and highlight those cases. Then the developer has to either

1. manually modify the auto-generated SQL statements or create new ones
2. write a data migration in Go

#### Manually writing a migration from scratch or modifying an existing one

If the autogenerated schema migration isn't too far off and only data conversion
needs to happen, the existing SQL statement can be modified or amended with
manually written SQL code. This can involve adding a `USING <expression>`
statement to the `ALTER TABLE` clause to facilitate data conversion. Resources:
- https://stackoverflow.com/q/19559851
- https://stackoverflow.com/questions/7162903/how-to-alter-a-columns-data-type-in-a-postgresql-table#comment35877191_7162961
- https://www.postgresql.org/docs/14/sql-altertable.html
- https://entgo.io/docs/data-migrations#versioned-migrations
- https://atlasgo.io/lint/analyzers
- https://atlasgo.io/versioned/troubleshoot


> Example - adding an `UNIQUE INDEX` to a non-unique column

Let's say you want to add a unique constraint to a previously unconstrained
column/field. Atlas will happily generate a suitable schema transformation that
will work on an empty database:

```sql
-- Create index "host_resources_device_guid_key" to table: "host_resources"
CREATE UNIQUE INDEX "host_resources_device_guid_key" ON "host_resources" ("device_guid");
```

But because non-unique data has been added to a production database, that same
migration will fail to apply in the prod setup:

```sql
postgres=# CREATE UNIQUE INDEX "host_resources_device_guid_key" ON "host_resources" ("device_guid");
ERROR:  could not create unique index "host_resources_device_guid_key"
DETAIL:  Key (device_guid)=(guid-1) is duplicated.
```

These duplicate values need to be removed or otherwise consolidated, before an
unique index can be created. In this instance, we find them and set all of them
to NULL, by adding an additional SQL statement to the generated migration:

```sql
-- Set duplicate "device_guid" rows to NULL.
UPDATE "host_resources" a SET "device_guid" = NULL FROM "host_resources" b WHERE a.id < b.id AND a.device_guid = b.device_guid;
-- Create index "host_resources_device_guid_key" to table: "host_resources"
CREATE UNIQUE INDEX "host_resources_device_guid_key" ON "host_resources" ("device_guid");
```

Should such a modification not be feasible, a migration can be written entirely
from scratch too:

```bash
make migration-new MIGRATION=<migration-name>
```

**Remember to update the hash file when done as [described](#re-hashing-the-atlassum-file).**

#### Generating a migration from Go code

Instead of hand-writing SQL like shown above, developers can write Go code
leveraging the Ent provided interface to describe the desired modifications
instead. The Ent Diff planner will then generate the SQL statements from that:
https://entgo.io/docs/data-migrations#generated-scripts. See the
[driver code](/internal/ent/migrate/gen_code_migrations.go) for further
instructions and an example.

### Re-hashing the `atlas.sum` file

Whenever migrations are manually modified, added or removed, the `atlas.sum`
checksum file needs to be updated to reflect the modified state. This can be
triggered with the following make target:

```bash
make migration-hash
```

### Validating Generated Scripts

The Atlas tooling also includes a linter which can provide validation of the
generated migration scripts and can serve as a means to alert the developer or
DB admin about possible problems in the upcoming migration. To run this
validation, simply execute this command:

```bash
make migration-lint
```

The output of this command may include warning about potential issues, which may
need to be addressed by additional manually written code. If schema changes are
made deliberately, this should not be necessary, but sometime it may not be
entirely avoidable.

At this point, the developer or DB admin have all the collateral to apply
(or execute) the migration.

## Applying Migrations

A newer version of the inventory service will always attempt to migrate the
given database at startup using the migrations files provided with the required
`-migrationsDir` flag. In most cases this is simply a no-op and will not change
the schema at all. This process will abort if any errors are detected. In this
failed state, manual human intervention is required.
Later in the startup process, the inventory will check that the database schema
is congruent with the expected Ent schema. If this is not the case, inventory
will refuse to start.

Before applying the migration on a production system, it is a good idea to first
do a dry-run of the migration scripts on an off-line copy of the production
system to make sure there are no unexpected issues, which could
cause the migration to leave the production system in an unusable state.

Ent automatic migrations, not to be confused with the automatic application of
migrations, and versioned migrations are fundamentally incompatible and will
lead to broken database state when mixed together. **Hence, inventory only uses
versioned migrations.**

## More documentation

More detailed documentation is provided on [entgo.io versioned migrations] page.
The above page provides merely a summary and a distillation of concrete steps to
be executed in the context of this specific project and database drivers.


[Atlas]: https://atlasgo.io/getting-started/
[entgo.io]: https://entgo.io
[entgo.io versioned migrations]: https://entgo.io/docs/versioned-migrations/

## Troubleshooting

In the unfortunate case you face an incompatible change like the following:

```bash
20230612170303_ci-mig.sql: destructive changes detected:

	L4: Dropping non-virtual column "target"

20230612170303_ci-mig.sql: data dependent changes detected:

	L12: Adding a unique index "repeated_schedule_resources_resource_id_key" on table "repeated_schedule_resources" might fail in case column "resource_id" contains duplicate entries
```

You can look at the current change that is causing the issue. Then you can
leverage the `atlas.sum` to have the order list of the migrations. Note that
each migration is a diff on the schema. Removing the incompatible change will
solve the issue. In order to do that, you find first the migration file that is
incompatible, then you remove/modify the file.

**Remember to update the hash file when done as [described](#re-hashing-the-atlassum-file).**

Then you regenerate the new migration and lint the migration files all together.

### Deleting the schema and all data

In some cases the database is irreparably modified and cannot be recovered. Then
it has to be cleaned and the schema can be created from scratch. Atlas provides
a helper function to facilitate DB cleanup.

#### In a Local Setup

If there is a running inventory instance, run the atlas binary inside it:

```bash
docker exec -ti <inventory container> atlas schema clean --url "postgres://$PGUSER:$PGPASSWORD@$PGHOST:$PGPORT/$PGDATABASE?search_path=public&sslmode=$PGSSLMODE"
```

If there is no running container, start a new one and run atlas directly:

```bash
docker run --rm -ti --net=host --env PGUSER=admin --env PGPASSWORD=pass --env PGSSLMODE=disable --env PGPORT=5432 --env PGHOST=localhost --env PGDATABASE=postgres --entrypoint atlas amr-registry.caas.intel.com/one-intel-edge/maestro-i/inventory:main schema clean --url "postgres://$PGUSER:$PGPASSWORD@$PGHOST:$PGPORT/$PGDATABASE?search_path=public&sslmode=$PGSSLMODE"
```

#### In a Production Setup

In production, you can derive the data needed for the access from the k8s secret.

```bash
PGDATA=$(kubectl get secret -n maestro-iaas-system inventory-aurora-postgresql -o jsonpath='{.data.PGDATABASE}'| base64 -d)
PGHOST=$(kubectl get secret -n maestro-iaas-system inventory-aurora-postgresql -o jsonpath='{.data.PGHOST}'| base64 -d)
PGPSWD=$(kubectl get secret -n maestro-iaas-system inventory-aurora-postgresql -o jsonpath='{.data.PGPASSWORD}'| base64 -d)
PGUSER=$(kubectl get secret -n maestro-iaas-system inventory-aurora-postgresql -o jsonpath='{.data.PGUSER}'| base64 -d)
PGPORT=$(kubectl get secret -n maestro-iaas-system inventory-aurora-postgresql -o jsonpath='{.data.PGPORT}'| base64 -d)

kubectl exec -it -n maestro-iaas-system mi-inventory-0 -- atlas schema clean \
  --url "postgres://$PGUSER:$PGPWD@$PGHOST:$PGPORT/$PGDB?search_path=public&sslmode=require"
```

**THIS WILL DELETE ALL STORED DATA!**

### Access the database in a Production Setup

1. Derive required data for accessing the DB from K8S secrets:
    ```bash
    PGDATA=$(kubectl get secret -n maestro-iaas-system inventory-aurora-postgresql -o jsonpath='{.data.PGDATABASE}'| base64 -d)
    PGHOST=$(kubectl get secret -n maestro-iaas-system inventory-aurora-postgresql -o jsonpath='{.data.PGHOST}'| base64 -d)
    PGPSWD=$(kubectl get secret -n maestro-iaas-system inventory-aurora-postgresql -o jsonpath='{.data.PGPASSWORD}'| base64 -d)
    PGUSER=$(kubectl get secret -n maestro-iaas-system inventory-aurora-postgresql -o jsonpath='{.data.PGUSER}'| base64 -d)
    PGPORT=$(kubectl get secret -n maestro-iaas-system inventory-aurora-postgresql -o jsonpath='{.data.PGPORT}'| base64 -d)
    ```

2. Run a Postgres container in the cluster:
    ```bash
   kubectl run psql --image=postgres:14.6-alpine --restart=Never -i -q --rm -- sleep infinity >/dev/null 2>&1 &
   ```
3. Run a psql shell in the Postgres container connecting to the actual Postgres DB (AWS Aurora or Platform Postgres):
    ```bash 
    kubectl exec -it psql -- env PGPASSWORD="$PGPSWD" psql -h "$PGHOST" -p "$PGPORT" -U "$PGUSER" -d "$PGDATA"
    ```
   This will spawn a psql shell with full access to the DB.

    **NOTE: ANY QUERY IN PSQL IS DONE DIRECTLY AGAINST THE DB**
4. Run any psql command, for example:
   ```bash
   maestro-iaas-system-inventory=> \d
   List of relations
   Schema |                Name                |   Type   |               Owner
   --------+------------------------------------+----------+------------------------------------
   public | atlas_schema_revisions             | table    | maestro-iaas-system-inventory_user
   public | endpoint_resources                 | table    | maestro-iaas-system-inventory_user
   public | endpoint_resources_id_seq          | sequence | maestro-iaas-system-inventory_user
   ...
   ```
5. Run any SQL query, for example:
   ```bash
   maestro-iaas-system-inventory=> SELECT * FROM instance_resources;
   id |  resource_id  |        kind         |      description       |     desired_state      | current_state | vm_memory_bytes | vm_cpu_cores | vm_storage_bytes | instance_resource_host | instance_resource_user | instance_resource_os | status | status_detail
   ----+---------------+---------------------+------------------------+------------------------+---------------+-----------------+--------------+------------------+------------------------+------------------------+----------------------+--------+---------------
   1 | inst-fb4e21ad | INSTANCE_KIND_METAL | host-03a45e18-instance | INSTANCE_STATE_RUNNING |               |                 |              |                  |                      5 |                        |                    1 |        |
   (1 row)
   ```
6. Remember to delete the psql pod:
   ```bash
   kubectl delete pods psql
   ```
